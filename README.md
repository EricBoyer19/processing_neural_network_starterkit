# processing_neural_network_starterkit
Transposed to processing through java of the good material from matt mazur.

Here we just took relu activation function instead of the sigmoid, for obvious reasons, you will find over internet.

We also take care the derivative of the error in respect with the bias weights that influence also the total cost.

I know the code is not optimzed, and I should use matrix calculation approch to get rid of all this mess of floats I use.
Be kind enough, I'm working on a much more optimized version, which should not take long. A matter of time and interest.
By the way thanks to Daniel Shiffman, who shares a lot through the coding train channel, with a great spirit of sharing live 
his approach in limited time coding stuff.

Anyway, the aim here is to propose a starting kit which is not perfect and from nothing.

I suggest you go and see what's on the subject describe in youtube channel 3brown1blue,
which explains very clearly all the stuff you need to build your first neural network, starting from nowhere.
Hope this material will help newbees.

Have fun, and make of this material a better thing for the community, share itn and propose enhanced version.
