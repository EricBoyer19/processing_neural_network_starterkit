# processing_neural_network_starterkit
Transposed to processing through java of the good material from matt mazur.

Here we just took relu activation function instead of the sigmoid, for obvious reasons, you will find over internet.

We also take care the derivative of the error in respect with the bias weights that influence also the total cost.

I know the code is not optimzed, and I should use matrix calculation approch to get rid of all this mess of floats I use.
Be kind enough, I'm working on a much more optimized version, which should not take long. A matter of time and interest.

Anyway, the aim is to propose a starting kit from nothing, well at least from what I've learned.

I suggest you go and see what's on the subject describe in youtube channel 3brown1blue,
which explains very clearly all the stuff you need to build a first neural network starting from nowhere.
Hope this material will help newbees.

Have fun, and make of this material a better thing for the community and share it.
